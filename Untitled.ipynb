{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af661ce0-6cd1-4cdc-9037-e08b1bb3d7a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 48\u001b[0m\n\u001b[0;32m     31\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m args_simple_connexions_HAN_1 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist_mention\u001b[39m\u001b[38;5;124m\"\u001b[39m: list_mention,\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist_event\u001b[39m\u001b[38;5;124m\"\u001b[39m:list_event,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m }\n\u001b[1;32m---> 48\u001b[0m fake_news_detector \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleConnexionsHAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# fake_news_detector = CloseEventsConnexionsHAN(label,device) \u001b[39;00m\n\u001b[0;32m     51\u001b[0m fake_news_detector\u001b[38;5;241m.\u001b[39mcreate_graph_and_train_on_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs_simple_connexions_HAN_1)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "from SimplePreprocessing import SimplePreprocessing\n",
    "from Heterogemodel import HAN\n",
    "from Training import SimpleTraining\n",
    "from TrainingService import SimpleConnexionsHAN,CloseEventsConnexionsHAN\n",
    "\n",
    "\n",
    "from GraphViz import GraphViz\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "def get_csv_files(directory, n):\n",
    "    csv_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".CSV\"):\n",
    "            # print(os.path.join(directory, filename))\n",
    "            csv_files.append(os.path.join(directory, filename))\n",
    "            if len(csv_files) == n:\n",
    "                break\n",
    "    return csv_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    nb_event_csv = 100 # TODO: fail at 17 if label = source ( bug fixé mais un peu à la zob)\n",
    "    nb_mentions_csv = 100\n",
    "    list_mention = get_csv_files(\"gdelt_data\", nb_mentions_csv)\n",
    "    list_event = get_csv_files(\"gdelt_data_event\",nb_event_csv)\n",
    "\n",
    "    label = \"source\"\n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device ('cpu')\n",
    "    args_simple_connexions_HAN_1 = {\n",
    "    \"list_mention\": list_mention,\n",
    "    \"list_event\":list_event,\n",
    "    \n",
    "    \"hidden_channels\": 64,\n",
    "    \"out_channels\": 2,\n",
    "    \"n_heads\": 4,\n",
    "    \"dropout\": 0.5,\n",
    "    #\"nb_epoch\": [10,15,20],\n",
    "    #\"lr\": [0.1,0.005,0.001],\n",
    "    \"nb_epoch\": 10,\n",
    "    \"lr\" : 0.001,\n",
    "    \"weight_decay\":0.001\n",
    "    \n",
    "    }\n",
    "    \n",
    "    fake_news_detector = SimpleConnexionsHAN(label,device) \n",
    "    # fake_news_detector = CloseEventsConnexionsHAN(label,device) \n",
    "    \n",
    "    fake_news_detector.create_graph_and_train_on_model(**args_simple_connexions_HAN_1)\n",
    "    \n",
    "    # TODO: create a function to save model\n",
    "    # TODO faire en sorte que les resultats soient reproductibles\n",
    "    \n",
    "    # analyse = GraphViz(label,list_event,list_mention)\n",
    "    # analyse.get_recap()\n",
    "    # analyse.display_graph()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ab64d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bilel\\PycharmProjects\\FakeNews\\fake-news-detection\\Preprocessing.py:166: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  est_source_de[self.non_label_column] = est_source_de[self.non_label_column].map(article_map[\"index\"]).astype(int)\n",
      "c:\\Users\\bilel\\PycharmProjects\\FakeNews\\fake-news-detection\\Preprocessing.py:168: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  est_source_de[self.label_column] = est_source_de[self.label_column].map(source_map[\"index\"]) # Bugfix attempt, may be suboptimal\n",
      "c:\\Users\\bilel\\PycharmProjects\\FakeNews\\fake-news-detection\\Preprocessing.py:181: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mentionné[\"MentionIdentifier\"] = mentionné[\"MentionIdentifier\"].map(article_map[\"index\"]).astype(int)\n",
      "c:\\Users\\bilel\\PycharmProjects\\FakeNews\\fake-news-detection\\Preprocessing.py:188: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mentionné[\"GlobalEventID\"] = mentionné[\"GlobalEventID\"].map(event_map[\"index\"])\n",
      "c:\\Users\\bilel\\anaconda3\\envs\\DeepL\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bilel\\anaconda3\\envs\\DeepL\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bilel\\anaconda3\\envs\\DeepL\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bilel\\anaconda3\\envs\\DeepL\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bilel\\anaconda3\\envs\\DeepL\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bilel\\anaconda3\\envs\\DeepL\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001, Epochs: 10, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.001, Epochs: 15, Result(Precision,Recall,F1 score): (0.125, 1.0, 0.2222222222222222)\n",
      "Learning Rate: 0.001, Epochs: 20, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.001, Epochs: 50, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.001, Epochs: 100, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.001, Epochs: 500, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.005, Epochs: 10, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.005, Epochs: 15, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.005, Epochs: 20, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.005, Epochs: 50, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.005, Epochs: 100, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.005, Epochs: 500, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.01, Epochs: 10, Result(Precision,Recall,F1 score): (0.14285714285714285, 1.0, 0.25)\n",
      "Learning Rate: 0.01, Epochs: 15, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.01, Epochs: 20, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.01, Epochs: 50, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.01, Epochs: 100, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.01, Epochs: 500, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.1, Epochs: 10, Result(Precision,Recall,F1 score): (0.1, 1.0, 0.18181818181818182)\n",
      "Learning Rate: 0.1, Epochs: 15, Result(Precision,Recall,F1 score): (0.1, 1.0, 0.18181818181818182)\n",
      "Learning Rate: 0.1, Epochs: 20, Result(Precision,Recall,F1 score): (0.1, 1.0, 0.18181818181818182)\n",
      "Learning Rate: 0.1, Epochs: 50, Result(Precision,Recall,F1 score): (0.1111111111111111, 1.0, 0.19999999999999998)\n",
      "Learning Rate: 0.1, Epochs: 100, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n",
      "Learning Rate: 0.1, Epochs: 500, Result(Precision,Recall,F1 score): (0.0, 0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "from SimplePreprocessing import SimplePreprocessing\n",
    "from Heterogemodel import HAN\n",
    "from Training import SimpleTraining\n",
    "from TrainingService import SimpleConnexionsHAN,CloseEventsConnexionsHAN,EmbeddedFeaturesEventHAN\n",
    "from GraphViz import GraphViz\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_csv_files(directory, n):\n",
    "    csv_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".CSV\"):\n",
    "            # print(os.path.join(directory, filename))\n",
    "            csv_files.append(os.path.join(directory, filename))\n",
    "            if len(csv_files) == n:\n",
    "                break\n",
    "    return csv_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    nb_event_csv = 1 # TODO: fail at 17 if label = source ( bug fixé mais un peu à la zob)\n",
    "    nb_mentions_csv = 1\n",
    "    list_mention = get_csv_files(\"gdelt_data\", nb_mentions_csv)\n",
    "    list_event = get_csv_files(\"gdelt_data_event\",nb_event_csv)\n",
    "\n",
    "    label = \"source\"\n",
    "    is_mixte = False\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    args_simple_connexions_HAN_1 = {\n",
    "    \"list_mention\": list_mention,\n",
    "    \"list_event\":list_event,\n",
    "    \n",
    "    \"hidden_channels\": 64,\n",
    "    \"out_channels\": 2,\n",
    "    \"n_heads\": 4,\n",
    "    \"dropout\": 0.2,\n",
    "    \"nb_epoch\": [10,15,20,50,100,500],\n",
    "    \"lr\": [0.001,0.005,0.01,0.1],\n",
    "    \"weight_decay\":0.001\n",
    "    }\n",
    "    \n",
    "    name_save = \"test\"\n",
    "    name_load = \"test\"\n",
    "    list_arg_save_graph = [\"list_mention\",'list_event']\n",
    "    list_arg_load_graph = [\"hidden_channels\",\"out_channels\",\"n_heads\",\"dropout\",\"nb_epoch\",\"lr\",\"weight_decay\"]\n",
    "    args_save_graph = {key:args_simple_connexions_HAN_1[key] for key in list_arg_save_graph}\n",
    "    args_load_graph = {key:args_simple_connexions_HAN_1[key] for key in list_arg_load_graph}\n",
    "\n",
    "       \n",
    "    #fake_news_detector = SimpleConnexionsHAN(label,is_mixte,device) \n",
    "    #fake_news_detector = CloseEventsConnexionsHAN(label,is_mixte,device,col=\"Actor1Name\") \n",
    "    fake_news_detector = EmbeddedFeaturesEventHAN(label,is_mixte,device)\n",
    "    \n",
    "    fake_news_detector.create_graph_and_train_on_model(**args_simple_connexions_HAN_1)\n",
    "    # fake_news_detector.create_graph_and_save(**args_save_graph,name = name_save)\n",
    "    # fake_news_detector.import_graph_and_train_on_model(**args_load_graph,name = name_load)\n",
    "\n",
    "\n",
    "    # TODO créer plus d'analyse pour le training (plot loss and val loss)\n",
    "    # TODO tester différents hyperparamètres\n",
    "    # TODO tester différents models( est ce que celui la n'est pas trop gros?)\n",
    "    # TODO avec des features ça serait sans doute mieux\n",
    "    # TODO tester ce qu'il se passerait si on mettait les mixed comme des fakes news pour rééquilibrer\n",
    "    \n",
    "    # TODO find a way to reduce the time needed to create new connections (this is better now but still not perfect)\n",
    "    \n",
    "    # analyse = GraphViz(label,list_event,list_mention)\n",
    "    # analyse.get_recap()\n",
    "    # analyse.display_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0810d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6601adcf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
